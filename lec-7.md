# ğŸ“Š Data Science â€” Lecture 7 Summary

### 1. Recap

* Ï‡Â²-test (goodness of fit, independence).
* Degrees of freedom explanation.
* Errors in hypothesis testing (Type I & II).

---

### 2. Parameter Estimation

Two approaches to estimate unknown parameters (like probability **p** or mean **Î¼**):

1. **Maximum Likelihood Estimation (MLE):** purely data-driven.
2. **Maximum a Posteriori (MAP):** combines data with prior beliefs.

---

### 3. Maximum Likelihood Estimation (MLE)

* Have random samples Xâ‚, Xâ‚‚, â€¦, Xn from a distribution with parameter Î¸.
* Find Î¸ that maximizes the likelihood of observing the data.

**Examples:**

1. **Biased coin (Binomial):** Toss 10 times, observe 4 heads.

   * Candidate probabilities p = {â…“, Â½, â…”}.
   * Compute P(4 heads):

     * p=â…“ â†’ 0.228
     * p=Â½ â†’ 0.205
     * p=â…” â†’ 0.057
   * MLE = â…“ (most likely).

2. **Bernoulli (single trial):** MLE of p = fraction of successes in sample.

3. **Binomial (n trials):** MLE of p = total successes Ã· (n Ã— experiments).

4. **Geometric (trials until success):** MLE of p = 1 Ã· (average number of trials).

**Properties of MLE:**

* Easy to compute.
* Consistent (n â†’ âˆ â†’ estimate â†’ true Î¸).
* Can overfit.
* Not always unique.

---

### 4. Bayesâ€™ Rule Refresher

* **Prior:** P(H) = belief before seeing data.
* **Likelihood:** P(E|H) = probability of evidence given H.
* **Posterior:** P(H|E) = \[P(E|H) P(H)] / P(E).

**Example: Disease Test**

* Prior: 0.1% chance of disease.
* Test accuracy: 99%. False positive: 1%.
* Posterior after positive test â‰ˆ 9%.
* If prior updated to 9% and test repeated â†’ posterior â‰ˆ 90%.

ğŸ‘‰ Shows how evidence accumulates using Bayesâ€™ rule.

---

### 5. Monty Hall Problem (Bayes Application)

* 3 doors: 1 car, 2 goats.

1. You pick a door.
2. Monty opens another door (goat).
3. You can switch or stay.

* If stay â†’ win chance = 1/3.
* If switch â†’ win chance = 2/3.

ğŸ‘‰ Switching **doubles chance of winning**.

---

### 6. Maximum a Posteriori (MAP)

* Similar to MLE but includes **prior belief** about Î¸.
* Formula: Î¸MAP = argmax P(Î¸|Data) âˆ P(Data|Î¸) Ã— P(Î¸).

**Properties:**

* Avoids overfitting (prior acts as regularizer).
* As n â†’ âˆ, MAP â†’ MLE (data dominates).

**Analogy:**

* MLE = "believe only data."
* MAP = "believe data + prior experience."

---

## âœ… Key Takeaways

* **MLE:** estimates parameters by maximizing likelihood.
* **Bayesâ€™ Rule:** updates beliefs using evidence.
* **Monty Hall Problem:** example of conditional probability.
* **MAP:** combines MLE with prior â†’ more robust.

ğŸ‘‰ Lecture 7 introduces **parameter estimation**: how to move from hypothesis testing to actually estimating the values behind the data.
